{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suleiman-odeh/NLP_Project_Team16/blob/main/Gemma_2/zero_shot_direct_Gemma_2_9B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "VF_J31ndkLZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4e45ccd-e42a-46f8-d1cb-74cde0345ab0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "loading the model\n",
        "cleared the output since it cant be upload to github\n",
        "\"\"\"\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from huggingface_hub import login\n",
        "\n",
        "# logging using user access token\n",
        "login()\n",
        "\n",
        "#  Define 4-Bit Configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,              # Loading in 4-bit\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16, # Compute in 16-bit for speed, store in 4-bit\n",
        ")\n",
        "\n",
        "# Load model\n",
        "model_id = \"google/gemma-2-9b-it\"\n",
        "\n",
        "print(f\"Loading {model_id} in 4-bit...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vikywMBUkcTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "load the dataset and keeps only the TEST split.\n",
        "We only need test for zero-shot evaluation.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "DATA_FILE = \"QEvasion_cleaned.jsonl\"\n",
        "\n",
        "print(f\"Loading data from {DATA_FILE} ...\")\n",
        "df = pd.read_json(DATA_FILE, lines=True)\n",
        "\n",
        "test_df = df[df[\"split_type\"] == \"test\"].copy()\n",
        "print(f\"Test rows: {len(test_df)}\")\n",
        "\n",
        "# test\n",
        "print(\"\\nColumns:\", list(test_df.columns))\n",
        "print(test_df[[\"question\", \"cleaned_answer\", \"clarity_id\"]].head(2))"
      ],
      "metadata": {
        "id": "1vOsM2duqL06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e9806e1-1819-4e85-ed2d-e938ccd0e72f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from QEvasion_cleaned.jsonl ...\n",
            "Test rows: 308\n",
            "\n",
            "Columns: ['title', 'date', 'president', 'url', 'question_order', 'interview_question', 'interview_answer', 'gpt3.5_summary', 'gpt3.5_prediction', 'question', 'annotator_id', 'annotator1', 'annotator2', 'annotator3', 'inaudible', 'multiple_questions', 'affirmative_questions', 'index', 'clarity_label', 'evasion_label', 'split_type', 'cleaned_answer', 'clarity_id', 'evasion_id', 'annotator1_id', 'annotator2_id', 'annotator3_id']\n",
            "                                               question  \\\n",
            "3448   Inquiring about the status or information reg...   \n",
            "3449  Will you invite them to the White House to neg...   \n",
            "\n",
            "                                         cleaned_answer  clarity_id  \n",
            "3448  Well, the world has made it clear that these t...           1  \n",
            "3449  I think that anytime and anyplace that they ar...           1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "define the direct (3-class) prompt and a small parser.\n",
        "We keep the output format strict so evaluation is reliable.\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "\n",
        "def make_direct_prompt(question, answer):\n",
        "    return f\"\"\"You classify the interviewee response to a question.\n",
        "\n",
        "we use this:\n",
        "1. clear reply (answers what was asked)\n",
        "2. clear Non-Reply (does not answer/ declines)\n",
        "3. Ambiguous (partial answer/ too general)\n",
        "\n",
        "Return the taxonomy number: 1, 2, or 3.\n",
        "\n",
        "Answer: {answer}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Taxonomy code:\"\"\"\n",
        "\n",
        "# Map prompt taxonomy -> dataset clarity_id\n",
        "# 1 -> 0 (Clear Reply), 2 -> 2 (Non-Reply), 3 -> 1 (Ambivalent/Ambiguous)\n",
        "CODE_TO_CLARITY = {1: 0, 2: 2, 3: 1}\n",
        "\n",
        "def parse_direct_output(text):\n",
        "    t = str(text).strip().lower()\n",
        "\n",
        "    # First try to catch a clean digit (1/2/3)\n",
        "    m = re.search(r\"\\b([1-3])\\b\", t)\n",
        "    if m:\n",
        "        return CODE_TO_CLARITY[int(m.group(1))]\n",
        "\n",
        "    # Fallback if it prints words\n",
        "    if \"non-reply\" in t or \"non reply\" in t:\n",
        "        return 2\n",
        "    if \"clear reply\" in t:\n",
        "        return 0\n",
        "    if \"ambiguous\" in t or \"ambivalent\" in t:\n",
        "        return 1\n",
        "\n",
        "    return -1\n"
      ],
      "metadata": {
        "id": "dWOgoVfV4Eak"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "run zero-shot inference on the test split.\n",
        "We decode only the newly generated tokens.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "@torch.no_grad()\n",
        "def gemma_generate_label(prompt, max_new_tokens=3):\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=2048\n",
        "    ).to(model.device)\n",
        "\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        use_cache=True\n",
        "    )\n",
        "\n",
        "    # Decode only the new tokens (after the prompt)\n",
        "    new_tokens = out[0][inputs[\"input_ids\"].shape[1]:]\n",
        "    return tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "preds = []\n",
        "raw_outs = []\n",
        "y_true = test_df[\"clarity_id\"].astype(int).tolist()\n",
        "\n",
        "print(\"Starting Gemma-2 direct zero-shot on TEST...\")\n",
        "\n",
        "debug_n = 5\n",
        "for i, row in tqdm(test_df.reset_index(drop=True).iterrows(), total=len(test_df)):\n",
        "    prompt = make_direct_prompt(row[\"question\"], row[\"cleaned_answer\"])\n",
        "    raw = gemma_generate_label(prompt, max_new_tokens=3)\n",
        "    pred = parse_direct_output(raw)\n",
        "\n",
        "    if i < debug_n:\n",
        "        print(f\"\\n[DEBUG {i+1}] raw={repr(raw)} | parsed={pred} | true={int(row['clarity_id'])}\")\n",
        "\n",
        "    raw_outs.append(raw)\n",
        "    preds.append(pred)\n",
        "\n",
        "test_df[\"raw_output\"] = raw_outs\n",
        "test_df[\"pred_clarity_id\"] = preds\n",
        "\n",
        "print(\"\\nFinished.\")\n",
        "print(\"Parsing failures (-1):\", sum(p == -1 for p in preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NEwXdxf4NqG",
        "outputId": "1b3a4c7f-53e7-4ce7-9b92-14697591b793"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Gemma-2 direct zero-shot on TEST...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/308 [00:01<09:04,  1.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[DEBUG 1] raw='3' | parsed=1 | true=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 2/308 [00:02<05:47,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[DEBUG 2] raw='3' | parsed=1 | true=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 3/308 [00:03<05:21,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[DEBUG 3] raw='3' | parsed=1 | true=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|▏         | 4/308 [00:04<05:18,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[DEBUG 4] raw='3' | parsed=1 | true=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 5/308 [00:05<04:43,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[DEBUG 5] raw='3' | parsed=1 | true=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 308/308 [05:07<00:00,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finished.\n",
            "Parsing failures (-1): 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "print the evaluation metrics (macro + weighted).\n",
        "We ignore any rows we couldn't parse (-1).\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "valid_idx = [i for i, p in enumerate(preds) if p != -1]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DIRECT RESULTS | Gemma-2-9B-IT | Zero-shot | TEST\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if len(valid_idx) == 0:\n",
        "    print(\"No valid predictions parsed. Check the debug outputs and prompt format.\")\n",
        "else:\n",
        "    if len(valid_idx) < len(preds):\n",
        "        print(f\"Warning: {len(preds) - len(valid_idx)} predictions could not be parsed.\")\n",
        "\n",
        "    y_true_f = [y_true[i] for i in valid_idx]\n",
        "    y_pred_f = [preds[i] for i in valid_idx]\n",
        "\n",
        "    acc = accuracy_score(y_true_f, y_pred_f)\n",
        "\n",
        "    prec_macro = precision_score(y_true_f, y_pred_f, average=\"macro\", zero_division=0)\n",
        "    rec_macro  = recall_score(y_true_f, y_pred_f, average=\"macro\", zero_division=0)\n",
        "    f1_macro   = f1_score(y_true_f, y_pred_f, average=\"macro\")\n",
        "\n",
        "    prec_weighted = precision_score(y_true_f, y_pred_f, average=\"weighted\", zero_division=0)\n",
        "    rec_weighted  = recall_score(y_true_f, y_pred_f, average=\"weighted\", zero_division=0)\n",
        "    f1_weighted   = f1_score(y_true_f, y_pred_f, average=\"weighted\")\n",
        "\n",
        "    print(f\"Accuracy:           {acc:.4f}\")\n",
        "    print(\"-\"*30)\n",
        "    print(f\"Macro Precision:    {prec_macro:.4f}\")\n",
        "    print(f\"Macro Recall:       {rec_macro:.4f}\")\n",
        "    print(f\"Macro F1:           {f1_macro:.4f}\")\n",
        "    print(\"-\"*30)\n",
        "    print(f\"Weighted Precision: {prec_weighted:.4f}\")\n",
        "    print(f\"Weighted Recall:    {rec_weighted:.4f}\")\n",
        "    print(f\"Weighted F1:        {f1_weighted:.4f}\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    print(classification_report(\n",
        "        y_true_f,\n",
        "        y_pred_f,\n",
        "        target_names=[\"Clear (0)\", \"Ambivalent (1)\", \"Non-Reply (2)\"],\n",
        "        zero_division=0\n",
        "    ))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nv7bXpfZ4O3E",
        "outputId": "0e7d9159-fda5-483f-9bf0-9549deebb666"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "DIRECT RESULTS | Gemma-2-9B-IT | Zero-shot | TEST\n",
            "============================================================\n",
            "Accuracy:           0.6851\n",
            "------------------------------\n",
            "Macro Precision:    0.6211\n",
            "Macro Recall:       0.4059\n",
            "Macro F1:           0.3869\n",
            "------------------------------\n",
            "Weighted Precision: 0.7045\n",
            "Weighted Recall:    0.6851\n",
            "Weighted F1:        0.5920\n",
            "------------------------------------------------------------\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     Clear (0)       0.83      0.06      0.12        79\n",
            "Ambivalent (1)       0.70      0.98      0.81       206\n",
            " Non-Reply (2)       0.33      0.17      0.23        23\n",
            "\n",
            "      accuracy                           0.69       308\n",
            "     macro avg       0.62      0.41      0.39       308\n",
            "  weighted avg       0.70      0.69      0.59       308\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "save files so we can compare models later without rerunning.\n",
        "\"\"\"\n",
        "\n",
        "test_df.to_csv(\"full_test_dataset_zs_direct_gemma.csv\", index=True)\n",
        "print(\"Saved: full_test_dataset_zs_direct_gemma.csv\")\n",
        "\n",
        "mini_df = test_df[[\"pred_clarity_id\", \"clarity_id\", \"raw_output\"]].copy()\n",
        "mini_df.to_csv(\"predictions_comparison_direct_gemma.csv\", index=True)\n",
        "print(\"Saved: predictions_comparison_direct_gemma.csv\")\n"
      ],
      "metadata": {
        "id": "LDmwhcuM4X-X",
        "outputId": "a293e851-01b1-43fe-affc-c22da8bbdb72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: full_test_dataset_zs_direct_gemma.csv\n",
            "Saved: predictions_comparison_direct_gemma.csv\n"
          ]
        }
      ]
    }
  ]
}
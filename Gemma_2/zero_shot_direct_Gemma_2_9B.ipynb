{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suleiman-odeh/NLP_Project_Team16/blob/main/Gemma_2/zero_shot_direct_Gemma_2_9B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "VF_J31ndkLZ5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell is for loading the model\n",
        "I have run it already but cleared the output since it cant be upload to github\n",
        "\"\"\"\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from huggingface_hub import login\n",
        "\n",
        "# logging using user access token\n",
        "login()\n",
        "\n",
        "#  Define 4-Bit Configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,              # Loading in 4-bit\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16, # Compute in 16-bit for speed, store in 4-bit\n",
        ")\n",
        "\n",
        "# Load model\n",
        "model_id = \"google/gemma-2-9b-it\"\n",
        "\n",
        "print(f\"Loading {model_id} in 4-bit...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vikywMBUkcTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell loads the dataset and keeps only the TEST split.\n",
        "We only need test for zero-shot evaluation, but we keep the full df if you want quick checks.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "DATA_FILE = \"QEvasion_cleaned.jsonl\"\n",
        "\n",
        "print(f\"Loading data from {DATA_FILE} ...\")\n",
        "df = pd.read_json(DATA_FILE, lines=True)\n",
        "\n",
        "test_df = df[df[\"split_type\"] == \"test\"].copy()\n",
        "print(f\"Test rows: {len(test_df)}\")\n",
        "\n",
        "# Quick sanity check\n",
        "print(\"\\nColumns:\", list(test_df.columns))\n",
        "print(test_df[[\"question\", \"cleaned_answer\", \"clarity_id\"]].head(2))"
      ],
      "metadata": {
        "id": "1vOsM2duqL06"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
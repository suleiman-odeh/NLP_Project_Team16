{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suleiman-odeh/NLP_Project_Team16/blob/main/Gemma_2/zero_shot_indirect_Gemma_2_9B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "VF_J31ndkLZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c89e465-97a7-44e5-be6d-09ebe13c4d6e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR7zF3SNbtWX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRgD7trIrfcF"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This cell is for loading the model\n",
        "I have run it already but cleared the output since it cant be upload to github\n",
        "\"\"\"\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from huggingface_hub import login\n",
        "\n",
        "# logging using user access token\n",
        "login()\n",
        "\n",
        "#  Define 4-Bit Configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,              # Loading in 4-bit\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16, # Compute in 16-bit for speed, store in 4-bit\n",
        ")\n",
        "\n",
        "# Load model\n",
        "model_id = \"google/gemma-2-9b-it\"\n",
        "\n",
        "print(f\"Loading {model_id} in 4-bit...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell is for loading the data and mapping the evasion labels to clarity IDs.\n",
        "Also its for defining the prompt function based on the paper.\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "try:\n",
        "    df = pd.read_json('QEvasion_cleaned.jsonl', lines=True)\n",
        "    print(f\"Data loaded. Total rows: {len(df)}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: Upload 'QEvasion_cleaned.jsonl' first.\")\n",
        "\n",
        "# Maps the 9 evasion labels to the 3 clarity IDs (0, 1, 2)\n",
        "EVASION_TO_CLARITY_ID = {\n",
        "    'Explicit': 0,\n",
        "    'Implicit': 1, 'Dodging': 1, 'Deflection': 1, 'General': 1, 'Partial/half-answer': 1,\n",
        "    'Declining to answer': 2, 'Claims ignorance': 2, 'Clarification': 2\n",
        "}\n",
        "\n",
        "# prompt function based on the paper\n",
        "def create_gemma_indirect_prompt(question, answer):\n",
        "    system_instruction = \"\"\"Based on a segment of the interview in which the interviewer poses a series of questions,\n",
        "    classify the type of response provided by the interviewee for the following question using the following taxonomy\n",
        "    and then provide a chain of thought explanation for your decision:\n",
        "\n",
        "<Taxonomy>\n",
        "1. Explicit: The information requested is explicitly stated (in the requested form).\n",
        "2. Implicit: The information requested is given, but without being explicitly stated (not in the expected form).\n",
        "3. General: The information provided is too general/lacks the requested specificity.\n",
        "4. Partial/half-answer: Offers only a specific component of the requested information.\n",
        "5. Dodging: Ignoring the question altogether.\n",
        "6. Deflection: Starts on topic but shifts the focus and makes a different point than what is asked.\n",
        "7. Declining to answer: Acknowledges the question but directly or indirectly refusing to answer at the moment.\n",
        "8. Claims ignorance: The answerer claims/admits not to know the answer themselves.\n",
        "9. Clarification: Does not provide the requested information and asks for clarification.\n",
        "You are required to respond with a single term corresponding to the Taxonomy code and only.\n",
        "\n",
        "\n",
        "### Part of the interview ###\n",
        "\"\"\"\n",
        "\n",
        "    # Gemma-Specific Chat Template\n",
        "    prompt = f\"\"\"<start_of_turn>user\n",
        "{system_instruction}\n",
        "Question: \"{question}\"\n",
        "Answer: \"{answer}\"\n",
        "\n",
        "### Question ###\n",
        "\"{question}\"\n",
        "<end_of_turn>\n",
        "<start_of_turn>model\n",
        "Taxonomy code:\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "print(\"Setup Complete.\")"
      ],
      "metadata": {
        "id": "4bI9hfyO7Wad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff5cfcf3-c25c-42f3-dcae-e5eb93de5062"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded. Total rows: 3756\n",
            "Setup Complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell is for running the zero shot on the test data\n",
        "\"\"\"\n",
        "import torch\n",
        "import gc\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# take test data\n",
        "test_df = df[df['split_type'] == 'test'].copy()\n",
        "print(f\"Processing Test Set: {len(test_df)} samples\")\n",
        "\n",
        "predictions_evasion = []\n",
        "predictions_clarity = []\n",
        "raw_outputs = []\n",
        "\n",
        "# Inference Loop\n",
        "print(\"Starting Zero-Shot Inference ...\")\n",
        "model.eval()\n",
        "\n",
        "for index, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
        "    # Create Prompt\n",
        "    prompt_text = create_gemma_indirect_prompt(row['question'], row['cleaned_answer'])\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate\n",
        "    # more tokens allowed because of COT\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode\n",
        "    generated_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    clean_text = generated_text.strip()\n",
        "    raw_outputs.append(clean_text)\n",
        "\n",
        "    # since the output might be a large text, we search, which word appeared from the evasion list in the text\n",
        "    detected_label = \"Error\"\n",
        "\n",
        "    found_labels = []\n",
        "    for valid_label in EVASION_TO_CLARITY_ID.keys():\n",
        "        # # If multiple labels found, pick the first one mentioned.\n",
        "        if re.search(r'\\b' + re.escape(valid_label) + r'\\b', clean_text, re.IGNORECASE):\n",
        "            found_labels.append(valid_label)\n",
        "\n",
        "    if found_labels:\n",
        "        best_match = min(found_labels, key=lambda l: clean_text.lower().find(l.lower()))\n",
        "        detected_label = best_match\n",
        "    else:\n",
        "        detected_label = \"Error\"\n",
        "\n",
        "    # Map to Clarity ID\n",
        "    mapped_id = EVASION_TO_CLARITY_ID.get(detected_label, -1)\n",
        "\n",
        "    predictions_evasion.append(detected_label)\n",
        "    predictions_clarity.append(mapped_id)\n",
        "\n",
        "# store results in the dataset for the evaultion in the next cell\n",
        "test_df['raw_output'] = raw_outputs\n",
        "test_df['pred_evasion'] = predictions_evasion\n",
        "test_df['pred_clarity_id'] = predictions_clarity\n",
        "\n",
        "print(\"Results are stored in the 'test_df' variable.\")\n"
      ],
      "metadata": {
        "id": "wmhRd1Nm8Il3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d72e0dbe-523e-4706-abec-ab73f1cc4438"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Test Set: 308 samples\n",
            "Starting Zero-Shot Inference ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 308/308 [40:18<00:00,  7.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results are stored in the 'test_df' variable.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Our results contained from the zeroshot included listing numbers and, sometimes, listing text. So, at the beginning,\n",
        "we need to convert the numbers into their corresponding evasion_label so that we can evaluate them.\n",
        "\"\"\"\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# ==========================================\n",
        "# DEFINE THE MAPS\n",
        "# ==========================================\n",
        "CODE_TO_ID = {\n",
        "    '1': 5, # Explicit\n",
        "    '2': 7, # Implicit\n",
        "    '3': 6, # General\n",
        "    '4': 8, # Partial/half-answer\n",
        "    '5': 4, # Dodging\n",
        "    '6': 3, # Deflection\n",
        "    '7': 2, # Declining to answer\n",
        "    '8': 0, # Claims ignorance\n",
        "    '9': 1  # Clarification\n",
        "}\n",
        "\n",
        "# ID -> Text Label\n",
        "ID_TO_LABEL = {\n",
        "    5: 'Explicit',\n",
        "    7: 'Implicit',\n",
        "    6: 'General',\n",
        "    8: 'Partial/half-answer',\n",
        "    4: 'Dodging',\n",
        "    3: 'Deflection',\n",
        "    2: 'Declining to answer',\n",
        "    0: 'Claims ignorance',\n",
        "    1: 'Clarification'\n",
        "}\n",
        "\n",
        "# Text -> ID\n",
        "WORD_TO_ID = {\n",
        "    'explicit': 5,\n",
        "    'implicit': 7,\n",
        "    'general': 6,\n",
        "    'partial': 8, 'half': 8,\n",
        "    'dodging': 4,\n",
        "    'deflection': 3,\n",
        "    'declining': 2,\n",
        "    'ignorance': 0, 'claims': 0,\n",
        "    'clarification': 1\n",
        "}\n",
        "\n",
        "# Clarity mapping\n",
        "def get_clarity_id(d_id):\n",
        "    if d_id == 5: return 0  # Explicit\n",
        "    if d_id in [7, 4, 3, 6, 8]: return 1 # Implicit, Dodging, Deflection, General, Partial\n",
        "    if d_id in [2, 0, 1]: return 2 # Declining, Ignorance, Clarification\n",
        "    return -1\n",
        "\n",
        "CLARITY_LABELS = {0: 'Clear Reply', 1: 'Ambivalent', 2: 'Clear Non-Reply'}\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# create new colimns\n",
        "# ==========================================\n",
        "def parse_model_output(row):\n",
        "    text = str(row['raw_output']).strip().lower()\n",
        "\n",
        "    # 1. Try Code (e.g. \"6\")\n",
        "    match = re.match(r'^(\\d)\\b', text)\n",
        "    if match:\n",
        "        return CODE_TO_ID.get(match.group(1), -1)\n",
        "\n",
        "    # 2. Try Word keywords\n",
        "    for word, id_num in WORD_TO_ID.items():\n",
        "        if word in text:\n",
        "            return id_num\n",
        "\n",
        "    return -1\n",
        "\n",
        "print(\"Processing predictions...\")\n",
        "test_df['pred_evasion_id'] = test_df.apply(parse_model_output, axis=1)\n",
        "\n",
        "# getting strings\n",
        "test_df['pred_evasion'] = test_df['pred_evasion_id'].map(ID_TO_LABEL)\n",
        "\n",
        "# Generate Clarity\n",
        "test_df['pred_clarity_id'] = test_df['pred_evasion_id'].apply(get_clarity_id)\n",
        "test_df['pred_clarity'] = test_df['pred_clarity_id'].map(CLARITY_LABELS)\n",
        "\n",
        "# Filter valid rows\n",
        "valid_df = test_df[test_df['pred_evasion_id'] != -1].copy()\n",
        "print(f\"Total Samples: {len(test_df)}\")\n",
        "print(f\"Valid Samples: {len(valid_df)}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# EVASION EVALUATION\n",
        "# ==========================================\n",
        "print(\"TASK 2: EVASION (9-Class)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def get_ground_truth(row):\n",
        "    pred = row['pred_evasion_id']\n",
        "    humans = [row['annotator1_id'], row['annotator2_id'], row['annotator3_id']]\n",
        "\n",
        "    valid_humans = []\n",
        "    for h in humans:\n",
        "        try:\n",
        "            if pd.notna(h): valid_humans.append(int(h))\n",
        "        except: pass\n",
        "\n",
        "    if pred in valid_humans:\n",
        "        return pred\n",
        "    else:\n",
        "        return valid_humans[0] if valid_humans else -1\n",
        "\n",
        "# Create Ground Truth\n",
        "valid_df['true_evasion_id'] = valid_df.apply(get_ground_truth, axis=1)\n",
        "\n",
        "# Filter for scoring\n",
        "final_df = valid_df[valid_df['true_evasion_id'] != -1].copy()\n",
        "\n",
        "y_true = final_df['true_evasion_id']\n",
        "y_pred = final_df['pred_evasion_id']\n",
        "\n",
        "# --- Metrics ---\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Macro\n",
        "prec_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "rec_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "# Weighted\n",
        "prec_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "rec_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "# --- Print ---\n",
        "print(f\"Accuracy:           {acc:.4f}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Macro Precision:    {prec_macro:.4f}\")\n",
        "print(f\"Macro Recall:       {rec_macro:.4f}\")\n",
        "print(f\"Macro F1:           {f1_macro:.4f}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Weighted Precision: {prec_weighted:.4f}\")\n",
        "print(f\"Weighted Recall:    {rec_weighted:.4f}\")\n",
        "print(f\"Weighted F1:        {f1_weighted:.4f}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Target Names\n",
        "target_names = [\n",
        "    \"Claims ignorance (0)\",\n",
        "    \"Clarification (1)\",\n",
        "    \"Declining to answer (2)\",\n",
        "    \"Deflection (3)\",\n",
        "    \"Dodging (4)\",\n",
        "    \"Explicit (5)\",\n",
        "    \"General (6)\",\n",
        "    \"Implicit (7)\",\n",
        "    \"Partial/half-answer (8)\"\n",
        "]\n",
        "\n",
        "print(classification_report(y_true, y_pred, labels=range(9), target_names=target_names))"
      ],
      "metadata": {
        "id": "MFdlCrCj89XB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8657dc6d-a9aa-4206-e08c-ce0bb708cc98"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing predictions...\n",
            "Total Samples: 308\n",
            "Valid Samples: 308\n",
            "------------------------------------------------------------\n",
            "TASK 2: EVASION (9-Class) PERFORMANCE\n",
            "============================================================\n",
            "Accuracy:           0.3312\n",
            "------------------------------\n",
            "Macro Precision:    0.5337\n",
            "Macro Recall:       0.3021\n",
            "Macro F1:           0.3076\n",
            "------------------------------\n",
            "Weighted Precision: 0.5183\n",
            "Weighted Recall:    0.3312\n",
            "Weighted F1:        0.3179\n",
            "------------------------------------------------------------\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "   Claims ignorance (0)       0.00      0.00      0.00         9\n",
            "      Clarification (1)       1.00      0.50      0.67         4\n",
            "Declining to answer (2)       1.00      0.20      0.33        10\n",
            "         Deflection (3)       0.21      0.93      0.34        43\n",
            "            Dodging (4)       0.57      0.08      0.14        51\n",
            "           Explicit (5)       0.61      0.28      0.39        95\n",
            "            General (6)       0.71      0.19      0.30        26\n",
            "           Implicit (7)       0.48      0.33      0.39        60\n",
            "Partial/half-answer (8)       0.22      0.20      0.21        10\n",
            "\n",
            "               accuracy                           0.33       308\n",
            "              macro avg       0.53      0.30      0.31       308\n",
            "           weighted avg       0.52      0.33      0.32       308\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter\n",
        "correct_df = test_df[test_df['pred_evasion'] != \"Error\"].copy()\n",
        "\n",
        "print(f\"Showing 6 samples of SUCCESSFUL extractions (Total: {len(correct_df)})\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Display raw output vs extracted label\n",
        "cols_to_show = ['raw_output', 'pred_evasion']\n",
        "print(correct_df[cols_to_show].head(6))"
      ],
      "metadata": {
        "id": "S2ApijgGoaNw",
        "outputId": "69b07acf-6b74-4954-ba84-eef6265a13c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Showing 6 samples of SUCCESSFUL extractions (Total: 308)\n",
            "--------------------------------------------------------------------------------\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                raw_output  \\\n",
            "3448                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Deflection   \n",
            "3449                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Deflection   \n",
            "3450  Deflection \\n\\n\\n### Chain of Thought Explanation:\\n\\nThe interviewee does not directly address the question of why Japan dropped the threat of sanctions. Instead, they provide a lengthy explanation about the broader diplomatic strategy regarding North Korea, emphasizing the importance of a unified message from the international community and the need for North Korea to choose a path of denuclearization.  The answer focuses on the goals of the UN Security Council resolution and the diplomatic efforts undertaken by various world leaders, effectively sidestepping the specific   \n",
            "3451                                                                 Deflection \\n\\n\\n### Chain of thought explanation:\\n\\nThe interviewee avoids directly answering the question about the timeline for the resolution. Instead, they shift the focus to:\\n\\n* Discussing the details of the resolution and its contents.\\n* Highlighting the concerns of various parties involved.\\n* Emphasizing the need for a multi-step process involving multiple resolutions.\\n* Mentioning ongoing diplomatic efforts and consultations.\\n\\nWhile they express a desire for a \"quick\" resolution, they provide no   \n",
            "3452                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Deflection   \n",
            "3453                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Deflection   \n",
            "\n",
            "     pred_evasion  \n",
            "3448   Deflection  \n",
            "3449   Deflection  \n",
            "3450   Deflection  \n",
            "3451   Deflection  \n",
            "3452   Deflection  \n",
            "3453   Deflection  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "save results for comparison\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "comparison_df = test_df[['pred_evasion', 'annotator1', 'annotator2', 'annotator3']].copy()\n",
        "\n",
        "comparison_df.to_csv(\"predictions_comparison.csv\", index=True)\n",
        "\n",
        "print(\"Saved 'predictions_comparison.csv' (Contains: Index, Prediction, 3 Annotators)\")\n",
        "\n",
        "\n",
        "test_df.to_csv(\"full_test_dataset_zs_indirect.csv\", index=True)\n",
        "\n",
        "print(\"Saved 'full_test_dataset_zs_indirect.csv'\")"
      ],
      "metadata": {
        "id": "3JhqCGysyYuV",
        "outputId": "92b8cead-8d9e-488b-cf1d-1058d974dc9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 'predictions_comparison.csv' (Contains: Index, Prediction, 3 Annotators)\n",
            "Saved 'full_test_dataset_zs_indirect.csv'\n"
          ]
        }
      ]
    }
  ]
}
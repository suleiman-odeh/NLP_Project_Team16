{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suleiman-odeh/NLP_Project_Team16/blob/main/Gemma_2/zero_shot_indirect_Gemma_2_9B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "VF_J31ndkLZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d5bedd8-3320-42c6-c314-33b3edd25703"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR7zF3SNbtWX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Load the Cleaned Data\n",
        "try:\n",
        "    df = pd.read_json('QEvasion_cleaned.jsonl', lines=True)\n",
        "    print(f\"Data loaded successfully! Total rows: {len(df)}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: Could not find 'QEvasion_cleaned.jsonl'. Please upload it to Colab files.\")\n",
        "\n",
        "\n",
        "# This maps the 9 evasion labels to the 3 clarity IDs (0, 1, 2)\n",
        "# Source: Project Proposal & Paper Taxonomy\n",
        "EVASION_TO_CLARITY_ID = {\n",
        "    # 0: Clear Reply\n",
        "    'Explicit': 0,\n",
        "\n",
        "    # 1: Ambivalent Reply\n",
        "    'Implicit': 1,\n",
        "    'Dodging': 1,\n",
        "    'Deflection': 1,\n",
        "    'General': 1,\n",
        "    'Partial/half-answer': 1,\n",
        "\n",
        "    # 2: Clear Non-Reply\n",
        "    'Declining to answer': 2,\n",
        "    'Claims ignorance': 2,\n",
        "    'Clarification': 2\n",
        "}\n",
        "\n",
        "# --- Step 3: Define the Gemma-Specific Prompt Function ---\n",
        "def create_gemma_indirect_prompt(question, answer):\n",
        "    \"\"\"\n",
        "    Creates a Zero-Shot prompt for Gemma-2-9b-It.\n",
        "    Includes definitions of the 9 labels to help the model distinguish them.\n",
        "    \"\"\"\n",
        "    system_instruction = \"\"\"You are an expert political discourse analyst.\n",
        "Your task is to classify the relationship between a Question and an Answer into exactly one of the following 9 categories.\n",
        "\n",
        "Definitions:\n",
        "1. Explicit: The information requested is explicitly stated.\n",
        "2. Implicit: The answer is implied but not explicitly stated.\n",
        "3. General: The answer is too general or lacks specificity.\n",
        "4. Partial/half-answer: Only addresses part of the question.\n",
        "5. Dodging: Ignores the question entirely.\n",
        "6. Deflection: Shifts focus to a related but different topic.\n",
        "7. Declining to answer: Refuses to answer (directly or indirectly).\n",
        "8. Claims ignorance: Claims not to know the answer.\n",
        "9. Clarification: Asks for clarification instead of answering.\n",
        "\n",
        "Instructions:\n",
        "- Analyze the Question and Answer carefully.\n",
        "- Return ONLY the category name from the list above.\n",
        "- Do not add explanations or punctuation.\"\"\"\n",
        "\n",
        "    # Gemma uses a specific chat template: <start_of_turn>user ... <end_of_turn>model\n",
        "    prompt = f\"\"\"<start_of_turn>user\n",
        "{system_instruction}\n",
        "\n",
        "Question: \"{question}\"\n",
        "Answer: \"{answer}\"\n",
        "\n",
        "Classify the answer:<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "print(\"Setup complete. Prompt function and Mapping dictionary are ready.\")"
      ],
      "metadata": {
        "id": "4bI9hfyO7Wad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Step 1: Prepare Test Data ---\n",
        "# We filter only the rows meant for testing\n",
        "test_df = df[df['split_type'] == 'test'].copy()\n",
        "print(f\"Processing Test Set: {len(test_df)} samples\")\n",
        "\n",
        "# Storage for results\n",
        "predictions_evasion = []\n",
        "predictions_clarity = []\n",
        "\n",
        "# --- Step 2: Inference Loop ---\n",
        "print(\"Starting Zero-Shot Inference (Gemma-2-9B-It)...\")\n",
        "\n",
        "# Ensure model is in eval mode to disable dropout etc.\n",
        "model.eval()\n",
        "\n",
        "for index, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
        "    # A. Create Prompt\n",
        "    prompt_text = create_gemma_indirect_prompt(row['question'], row['cleaned_answer'])\n",
        "\n",
        "    # B. Tokenize\n",
        "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # C. Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=20,   # We only need a short label\n",
        "            do_sample=False,     # Deterministic (Greedy) for reproducibility\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # D. Decode & Clean Output\n",
        "    # We slice [inputs.input_ids.shape[1]:] to remove the input prompt from the output\n",
        "    generated_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "    # Clean up: remove whitespace, punctuation (like periods at the end)\n",
        "    pred_label = generated_text.strip().rstrip('.')\n",
        "\n",
        "    # E. Map to Clarity ID (Indirect Logic)\n",
        "    # We look up the predicted evasion string in your dictionary.\n",
        "    # If the model hallucinates a label not in the dict, we default to -1 (Error)\n",
        "    mapped_id = EVASION_TO_CLARITY_ID.get(pred_label, -1)\n",
        "\n",
        "    predictions_evasion.append(pred_label)\n",
        "    predictions_clarity.append(mapped_id)\n",
        "\n",
        "# --- Step 3: Save Results ---\n",
        "test_df['pred_evasion'] = predictions_evasion\n",
        "test_df['pred_clarity_id'] = predictions_clarity\n",
        "\n",
        "output_filename = \"gemma_indirect_predictions.csv\"\n",
        "test_df.to_csv(output_filename, index=False)\n",
        "print(f\"\\nInference complete. Results saved to '{output_filename}'.\")"
      ],
      "metadata": {
        "id": "wmhRd1Nm8Il3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suleiman-odeh/NLP_Project_Team16/blob/main/Llama_3/zero_shot_indirect_Llama_3_8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aR7zF3SNbtWX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e6479c9-05b3-4b7e-ad1a-8f532858ed94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install -U transformers bitsandbytes accelerate huggingface_hub scikit-learn tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Model loader (4-bit).\n",
        "Keep output cleared before pushing to GitHub.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from huggingface_hub import login\n",
        "\n",
        "login()\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "print(f\"Loading {model_id} in 4-bit...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "print(\"Model loaded.\")\n"
      ],
      "metadata": {
        "id": "uB2bej_N9Vvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Load QEvasion cleaned file and select test split.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "DATA_FILE = \"QEvasion_cleaned.jsonl\"\n",
        "\n",
        "df = pd.read_json(DATA_FILE, lines=True)\n",
        "test_df = df[df[\"split_type\"] == \"test\"].copy()\n",
        "\n",
        "print(\"Test rows:\", len(test_df))\n",
        "print(test_df[[\"question\", \"cleaned_answer\", \"annotator1_id\", \"annotator2_id\", \"annotator3_id\"]].head(2))\n"
      ],
      "metadata": {
        "id": "qo2P5ZbRCEko",
        "outputId": "1a1677af-03e5-4ea5-9168-7bd062e9d414",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test rows: 308\n",
            "                                               question  \\\n",
            "3448   Inquiring about the status or information reg...   \n",
            "3449  Will you invite them to the White House to neg...   \n",
            "\n",
            "                                         cleaned_answer  annotator1_id  \\\n",
            "3448  Well, the world has made it clear that these t...              4   \n",
            "3449  I think that anytime and anyplace that they ar...              3   \n",
            "\n",
            "      annotator2_id  annotator3_id  \n",
            "3448              6              4  \n",
            "3449              6              6  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Indirect prompt.\n",
        "We ask for a single number 1..9 to keep parsing stable.\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "\n",
        "# Map model output code dataset evasion_id\n",
        "CODE_TO_EVASION_ID = {\n",
        "    1: 5,  # Explicit\n",
        "    2: 7,  # Implicit\n",
        "    3: 6,  # General\n",
        "    4: 8,  # Partial/half-answer\n",
        "    5: 4,  # Dodging\n",
        "    6: 3,  # Deflection\n",
        "    7: 2,  # Declining to answer\n",
        "    8: 0,  # Claims ignorance\n",
        "    9: 1,  # Clarification\n",
        "}\n",
        "\n",
        "ID_TO_LABEL = {\n",
        "    0: \"Claims ignorance\",\n",
        "    1: \"Clarification\",\n",
        "    2: \"Declining to answer\",\n",
        "    3: \"Deflection\",\n",
        "    4: \"Dodging\",\n",
        "    5: \"Explicit\",\n",
        "    6: \"General\",\n",
        "    7: \"Implicit\",\n",
        "    8: \"Partial/half-answer\",\n",
        "}\n",
        "\n",
        "def make_indirect_prompt(question, answer):\n",
        "    return f\"\"\"You are given an interview question and answer.\n",
        "Classify the answer using exactly one of the following evasion strategies:\n",
        "\n",
        "1. Explicit\n",
        "2. Implicit\n",
        "3. General\n",
        "4. Partial/half-answer\n",
        "5. Dodging\n",
        "6. Deflection\n",
        "7. Declining to answer\n",
        "8. Claims ignorance\n",
        "9. Clarification\n",
        "\n",
        "Return ONLY the number (1-9). No explanation.\n",
        "\n",
        "Answer: {answer}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Taxonomy code:\"\"\"\n",
        "\n",
        "def parse_indirect_output(text):\n",
        "    t = str(text).strip().lower()\n",
        "\n",
        "    # Grab first digit 1..9 anywhere in the output\n",
        "    m = re.search(r\"\\b([1-9])\\b\", t)\n",
        "    if not m:\n",
        "        return -1\n",
        "\n",
        "    code = int(m.group(1))\n",
        "    return CODE_TO_EVASION_ID.get(code, -1)\n"
      ],
      "metadata": {
        "id": "TgKjk8uLCNrm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Zero-shot loop.\n",
        "We keep max_new_tokens small because we want only a digit.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "@torch.no_grad()\n",
        "def llama_generate(prompt, max_new_tokens=5):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        temperature=0.1,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        use_cache=True\n",
        "    )\n",
        "\n",
        "    # Decode only new tokens\n",
        "    gen = out[0][inputs[\"input_ids\"].shape[1]:]\n",
        "    return tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
        "\n",
        "pred_ids = []\n",
        "raw_outs = []\n",
        "\n",
        "print(\"Running Llama-3 indirect zero-shot...\")\n",
        "\n",
        "debug_n = 5\n",
        "for i, row in tqdm(test_df.reset_index(drop=True).iterrows(), total=len(test_df), disable=False):\n",
        "    prompt = make_indirect_prompt(row[\"question\"], row[\"cleaned_answer\"])\n",
        "    raw = llama_generate(prompt, max_new_tokens=5)\n",
        "    pred = parse_indirect_output(raw)\n",
        "\n",
        "    if i < debug_n:\n",
        "        print(f\"\\n[DEBUG {i+1}] raw={repr(raw)} | pred_id={pred} ({ID_TO_LABEL.get(pred,'?')})\")\n",
        "\n",
        "    raw_outs.append(raw)\n",
        "    pred_ids.append(pred)\n",
        "\n",
        "test_df[\"raw_output\"] = raw_outs\n",
        "test_df[\"pred_evasion_id\"] = pred_ids\n",
        "test_df[\"pred_evasion_label\"] = test_df[\"pred_evasion_id\"].map(ID_TO_LABEL)\n",
        "\n",
        "print(\"\\nDone.\")\n",
        "print(\"Parsing failures (-1):\", sum(p == -1 for p in pred_ids))"
      ],
      "metadata": {
        "id": "l5PuPNyeCQsq",
        "outputId": "395f64c6-d867-4bf5-f13e-1d4592883e5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Llama-3 indirect zero-shot...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/308 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "  0%|          | 1/308 [00:03<15:25,  3.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[DEBUG 1] raw='6' | pred_id=3 (Deflection)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 2/308 [00:05<12:39,  2.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[DEBUG 2] raw='4' | pred_id=8 (Partial/half-answer)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 3/308 [00:08<15:16,  3.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[DEBUG 3] raw='4' | pred_id=8 (Partial/half-answer)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|▏         | 4/308 [00:12<16:36,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[DEBUG 4] raw='4' | pred_id=8 (Partial/half-answer)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 5/308 [00:14<14:29,  2.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[DEBUG 5] raw='4' | pred_id=8 (Partial/half-answer)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 308/308 [19:12<00:00,  3.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Done.\n",
            "Parsing failures (-1): 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Evaluation using annotator1_id/annotator2_id/annotator3_id.\n",
        "Match-any scoring logic.\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "valid_df = test_df[test_df[\"pred_evasion_id\"] != -1].copy()\n",
        "print(\"Valid rows:\", len(valid_df), \"/\", len(test_df))\n",
        "\n",
        "def pick_ground_truth(row):\n",
        "    pred = int(row[\"pred_evasion_id\"])\n",
        "    humans = [row[\"annotator1_id\"], row[\"annotator2_id\"], row[\"annotator3_id\"]]\n",
        "\n",
        "    human_ids = []\n",
        "    for h in humans:\n",
        "        if pd.notna(h):\n",
        "            try:\n",
        "                human_ids.append(int(h))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    if pred in human_ids:\n",
        "        return pred\n",
        "    return human_ids[0] if human_ids else -1\n",
        "\n",
        "valid_df[\"true_evasion_id\"] = valid_df.apply(pick_ground_truth, axis=1)\n",
        "final_df = valid_df[valid_df[\"true_evasion_id\"] != -1].copy()\n",
        "\n",
        "y_true = final_df[\"true_evasion_id\"].astype(int)\n",
        "y_pred = final_df[\"pred_evasion_id\"].astype(int)\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "prec_macro = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "rec_macro  = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "f1_macro   = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "prec_weighted = precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
        "rec_weighted  = recall_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
        "f1_weighted   = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INDIRECT RESULTS | Llama-3-8B-Instruct | Zero-shot | TEST\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Accuracy:           {acc:.4f}\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Macro Precision:    {prec_macro:.4f}\")\n",
        "print(f\"Macro Recall:       {rec_macro:.4f}\")\n",
        "print(f\"Macro F1:           {f1_macro:.4f}\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Weighted Precision: {prec_weighted:.4f}\")\n",
        "print(f\"Weighted Recall:    {rec_weighted:.4f}\")\n",
        "print(f\"Weighted F1:        {f1_weighted:.4f}\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "target_names = [\n",
        "    \"Claims ignorance (0)\",\n",
        "    \"Clarification (1)\",\n",
        "    \"Declining to answer (2)\",\n",
        "    \"Deflection (3)\",\n",
        "    \"Dodging (4)\",\n",
        "    \"Explicit (5)\",\n",
        "    \"General (6)\",\n",
        "    \"Implicit (7)\",\n",
        "    \"Partial/half-answer (8)\",\n",
        "]\n",
        "\n",
        "print(classification_report(y_true, y_pred, labels=list(range(9)), target_names=target_names, zero_division=0))"
      ],
      "metadata": {
        "id": "wr3F1zLuCU_U",
        "outputId": "48057b42-d0eb-443b-fe67-13ccf4c675ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid rows: 308 / 308\n",
            "\n",
            "============================================================\n",
            "INDIRECT RESULTS | Llama-3-8B-Instruct | Zero-shot | TEST\n",
            "============================================================\n",
            "Accuracy:           0.2045\n",
            "------------------------------\n",
            "Macro Precision:    0.3012\n",
            "Macro Recall:       0.2174\n",
            "Macro F1:           0.1544\n",
            "------------------------------\n",
            "Weighted Precision: 0.4870\n",
            "Weighted Recall:    0.2045\n",
            "Weighted F1:        0.2005\n",
            "------------------------------------------------------------\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "   Claims ignorance (0)       0.25      0.36      0.30        11\n",
            "      Clarification (1)       0.00      0.00      0.00         4\n",
            "Declining to answer (2)       0.50      0.20      0.29        10\n",
            "         Deflection (3)       0.06      0.03      0.04        29\n",
            "            Dodging (4)       0.75      0.05      0.10        56\n",
            "           Explicit (5)       0.73      0.08      0.15        95\n",
            "            General (6)       0.00      0.00      0.00        23\n",
            "           Implicit (7)       0.39      0.55      0.46        74\n",
            "Partial/half-answer (8)       0.03      0.67      0.05         6\n",
            "\n",
            "               accuracy                           0.20       308\n",
            "              macro avg       0.30      0.22      0.15       308\n",
            "           weighted avg       0.49      0.20      0.20       308\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "evaluation derived from the 9-class evasion predictions.\n",
        "we map evasion_id to clarity_id then score to clarity_id.\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "def evasion_to_clarity(eid: int) -> int:\n",
        "    # clarity_id: 0 Clear Reply, 1 Ambivalent, 2 Clear Non-Reply\n",
        "    if eid == 5:                 # Explicit\n",
        "        return 0\n",
        "    if eid in [7, 6, 8, 4, 3]:   # Implicit\n",
        "        return 1\n",
        "    if eid in [2, 0, 1]:         # Declining\n",
        "        return 2\n",
        "    return -1\n",
        "\n",
        "# create predicted clarity from predicted evasion\n",
        "clar_df = test_df[test_df[\"pred_evasion_id\"] != -1].copy()\n",
        "clar_df[\"pred_clarity_id\"] = clar_df[\"pred_evasion_id\"].astype(int).apply(evasion_to_clarity)\n",
        "clar_df[\"true_clarity_id\"] = clar_df[\"clarity_id\"].astype(int)\n",
        "\n",
        "# safety filter should keep all rows\n",
        "clar_df = clar_df[clar_df[\"pred_clarity_id\"] != -1].copy()\n",
        "\n",
        "y_true_c = clar_df[\"true_clarity_id\"]\n",
        "y_pred_c = clar_df[\"pred_clarity_id\"]\n",
        "\n",
        "acc_c = accuracy_score(y_true_c, y_pred_c)\n",
        "\n",
        "prec_macro_c = precision_score(y_true_c, y_pred_c, average=\"macro\", zero_division=0)\n",
        "rec_macro_c  = recall_score(y_true_c, y_pred_c, average=\"macro\", zero_division=0)\n",
        "f1_macro_c   = f1_score(y_true_c, y_pred_c, average=\"macro\")\n",
        "\n",
        "prec_weighted_c = precision_score(y_true_c, y_pred_c, average=\"weighted\", zero_division=0)\n",
        "rec_weighted_c  = recall_score(y_true_c, y_pred_c, average=\"weighted\", zero_division=0)\n",
        "f1_weighted_c   = f1_score(y_true_c, y_pred_c, average=\"weighted\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TASK 1: CLARITY (3-Class) — mapped from indirect (9-class) predictions\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Accuracy:           {acc_c:.4f}\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Macro Precision:    {prec_macro_c:.4f}\")\n",
        "print(f\"Macro Recall:       {rec_macro_c:.4f}\")\n",
        "print(f\"Macro F1:           {f1_macro_c:.4f}\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Weighted Precision: {prec_weighted_c:.4f}\")\n",
        "print(f\"Weighted Recall:    {rec_weighted_c:.4f}\")\n",
        "print(f\"Weighted F1:        {f1_weighted_c:.4f}\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "print(classification_report(\n",
        "    y_true_c,\n",
        "    y_pred_c,\n",
        "    target_names=[\"Clear Reply (0)\", \"Ambivalent (1)\", \"Clear Non-Reply (2)\"],\n",
        "    zero_division=0\n",
        "))\n"
      ],
      "metadata": {
        "id": "dxjO53eUVh8Q",
        "outputId": "37d46a91-c737-4ab1-c87a-c122f1bc11e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TASK 1: CLARITY (3-Class) — mapped from indirect (9-class) predictions\n",
            "============================================================\n",
            "Accuracy:           0.6623\n",
            "------------------------------\n",
            "Macro Precision:    0.5092\n",
            "Macro Recall:       0.4358\n",
            "Macro F1:           0.4081\n",
            "------------------------------\n",
            "Weighted Precision: 0.6305\n",
            "Weighted Recall:    0.6623\n",
            "Weighted F1:        0.5905\n",
            "------------------------------------------------------------\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "    Clear Reply (0)       0.55      0.08      0.13        79\n",
            "     Ambivalent (1)       0.70      0.93      0.80       206\n",
            "Clear Non-Reply (2)       0.28      0.30      0.29        23\n",
            "\n",
            "           accuracy                           0.66       308\n",
            "          macro avg       0.51      0.44      0.41       308\n",
            "       weighted avg       0.63      0.66      0.59       308\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Save outputs for later comparison\n",
        "\"\"\"\n",
        "\n",
        "test_df.to_csv(\"full_test_dataset_zs_indirect_llama3.csv\", index=True)\n",
        "print(\"Saved: full_test_dataset_zs_indirect_llama3.csv\")\n",
        "\n",
        "cols = [\n",
        "    \"pred_evasion_id\",\n",
        "    \"pred_evasion_label\",\n",
        "    \"pred_clarity_id_from_indirect\",\n",
        "    \"clarity_id\",\n",
        "    \"annotator1_id\",\n",
        "    \"annotator2_id\",\n",
        "    \"annotator3_id\",\n",
        "    \"raw_output\",\n",
        "]\n",
        "\n",
        "# Keep only columns that actually exist (prevents KeyError)\n",
        "cols = [c for c in cols if c in test_df.columns]\n",
        "\n",
        "mini_df = test_df[cols].copy()\n",
        "mini_df.to_csv(\"predictions_comparison_indirect_llama3.csv\", index=True)\n",
        "print(\"Saved: predictions_comparison_indirect_llama3.csv\")\n",
        "print(\"Mini columns:\", cols)\n"
      ],
      "metadata": {
        "id": "_km-jOP5Cc10",
        "outputId": "164a5c72-6150-4f52-9dc9-d1f0cecffab3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: full_test_dataset_zs_indirect_llama3.csv\n",
            "Saved: predictions_comparison_indirect_llama3.csv\n",
            "Mini columns: ['pred_evasion_id', 'pred_evasion_label', 'clarity_id', 'annotator1_id', 'annotator2_id', 'annotator3_id', 'raw_output']\n"
          ]
        }
      ]
    }
  ]
}
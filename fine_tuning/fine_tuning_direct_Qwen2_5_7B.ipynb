{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN1ts5ZOZ7QLpA8idEhgU4a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suleiman-odeh/NLP_Project_Team16/blob/main/fine_tuning/fine_tuning_direct_Qwen2_5_7B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers peft trl bitsandbytes accelerate datasets"
      ],
      "metadata": {
        "id": "611qdMp81GHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AA22ubYTzqif",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c191c91-7cd4-4441-ce0d-aa94b40848c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ready to go.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Setup\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "\n",
        "print(\"Ready to go.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Base model\n",
        "model_name = \"Qwen/Qwen2.5-7B\"\n",
        "\n",
        "# Define 4-Bit Configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load Base Model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "# fix pad tokens\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Prepare for LoRA\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA Config according to the paper\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\", # text generation model\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "    )\n",
        "\n",
        "print(f\"Loaded BASE model: {model_name}\")"
      ],
      "metadata": {
        "id": "Xs227XcI3JaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load dataset\n",
        "filename = \"QEvasion_cleaned.jsonl\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_json(filename, lines=True)\n",
        "    print(f\"Loaded {len(df)} rows using Pandas.\")\n",
        "except ValueError:\n",
        "    # Fallback if it's a standard JSON list (not lines)\n",
        "    df = pd.read_json(filename)\n",
        "    print(f\"Loaded {len(df)} rows using Pandas (Standard JSON).\")\n",
        "\n",
        "# Filter train and test sets\n",
        "train_df = df[df['split_type'] == 'train']\n",
        "# We keep the test set safe for later\n",
        "test_df  = df[df['split_type'] == 'test']\n",
        "\n",
        "print(f\"   - Training pool: {len(train_df)}\")\n",
        "print(f\"   - Test pool: {len(test_df)}\")\n",
        "\n",
        "# convert to hugging face dataset\n",
        "full_train_dataset = Dataset.from_pandas(train_df)\n",
        "\n",
        "# SPLIT (TRAIN vs VALIDATION)\n",
        "# so it can match the paper\n",
        "dataset_split = full_train_dataset.train_test_split(test_size=0.2175, seed=42)\n",
        "\n",
        "train_dataset = dataset_split['train']\n",
        "eval_dataset = dataset_split['test']\n",
        "\n",
        "print(f\"Final Setup:\")\n",
        "print(f\"   - Training Samples: {len(train_dataset)}\")\n",
        "print(f\"   - Validation Samples: {len(eval_dataset)}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# prompt based on the paper\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    output_texts = []\n",
        "\n",
        "    # FIXED: Added \\n for formatting and changed \"Ambiguous\" to \"Ambivalent\"\n",
        "    instruction = (\n",
        "        \"Based on a part of the interview where the interviewer asks a set of questions, \"\n",
        "        \"classify the type of answer the interviewee provided for the following question \"\n",
        "        \"into one of these categories:\\n\"\n",
        "        \"1. Clear Reply - The information requested is explicitly stated (in the requested form)\\n\"\n",
        "        \"2. Clear Non-Reply - The information requested is not given at all due to ignorance, need for clarification or declining to answer\\n\"\n",
        "        \"3. Ambivalent - The information requested is given in an incomplete way e.g. the answer is too general, partial, implicit, dodging or deflection.\"\n",
        "    )\n",
        "\n",
        "    # Define columns\n",
        "    answers = examples['cleaned_answer']\n",
        "    questions = examples['question']\n",
        "    labels = examples['clarity_label']\n",
        "\n",
        "    for i in range(len(questions)):\n",
        "        # Get the label text\n",
        "        label_text = str(labels[i])\n",
        "\n",
        "        # Construct Prompt\n",
        "        text = f\"{instruction}\\n\\n### Part of the interview ###\\n{answers[i]}\\n\\n### Question ###\\n{questions[i]}\\n\\nLabel: {label_text}\"\n",
        "\n",
        "        # Add EOS token\n",
        "        text = text + tokenizer.eos_token\n",
        "        output_texts.append(text)\n",
        "\n",
        "    return output_texts\n",
        "\n",
        "# VERIFY\n",
        "print(\"\\n--- Sample Training Input ---\")\n",
        "print(formatting_prompts_func(train_dataset[:1])[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73YoViaO_xjq",
        "outputId": "0baee413-c2d9-4136-857f-2d7398da0022"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 3756 rows using Pandas.\n",
            "   - Training pool: 3448\n",
            "   - Test pool: 308\n",
            "Final Setup:\n",
            "   - Training Samples: 2698\n",
            "   - Validation Samples: 750\n",
            "\n",
            "--- Sample Training Input ---\n",
            "Based on a part of the interview where the interviewer asks a set of questions, classify the type of answer the interviewee provided for the following question into one of these categories:\n",
            "1. Clear Reply - The information requested is explicitly stated (in the requested form)\n",
            "2. Clear Non-Reply - The information requested is not given at all due to ignorance, need for clarification or declining to answer\n",
            "3. Ambivalent - The information requested is given in an incomplete way e.g. the answer is too general, partial, implicit, dodging or deflection.\n",
            "\n",
            "### Part of the interview ###\n",
            "Am I going to meet with the Iranians directly, is that the question?\n",
            "\n",
            "### Question ###\n",
            "Is it necessary to deal with Iranian directly?\n",
            "\n",
            "Label: Clear Non-Reply<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Training Loop\n",
        "\"\"\"\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# 1. DEFINE TRAINING ARGUMENTS\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen-finetuned-evasion\", # Where to save results\n",
        "    per_device_train_batch_size=2,         # Low batch size to prevent crashing\n",
        "    gradient_accumulation_steps=8,         # Accumulate to simulate batch_size=16\n",
        "    gradient_checkpointing=True,           # Saves huge memory (trades speed for RAM)\n",
        "    learning_rate=2e-4,                    # Standard QLoRA learning rate\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    max_steps=200,                         # 200 steps is enough for a strong demo\n",
        "    logging_steps=10,                      # Print stats every 10 steps\n",
        "    evaluation_strategy=\"steps\",           # Check Validation Set? YES\n",
        "    eval_steps=50,                         # Check validation every 50 steps\n",
        "    save_strategy=\"no\",                    # Don't save intermediate checkpoints (saves disk)\n",
        "    fp16=True,                             # Use 16-bit precision for speed\n",
        "    optim=\"paged_adamw_8bit\",              # 8-bit Optimizer (Crucial for T4)\n",
        "    report_to=\"none\"                       # Turn off WandB (keeps output clean)\n",
        ")\n",
        "\n",
        "# 2. INITIALIZE TRAINER\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,             # The validation set we created\n",
        "    peft_config=peft_config,\n",
        "    formatting_func=formatting_prompts_func, # Your prompt function\n",
        "    args=training_args,\n",
        "    max_seq_length=1024,                   # Cuts off really long interviews\n",
        "    packing=False                          # Keep samples separate\n",
        ")\n",
        "\n",
        "# 3. START TRAINING\n",
        "print(\"ðŸš€ Starting Training...\")\n",
        "trainer.train()\n",
        "\n",
        "# 4. SAVE THE ADAPTER\n",
        "trainer.model.save_pretrained(\"./final_adapter_qwen\")\n",
        "print(\"âœ… Training Complete. Adapter saved to './final_adapter_qwen'\")"
      ],
      "metadata": {
        "id": "bkDQ5pG2LwpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Inference: Check the testset\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LwvXsAznL6Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Evaluation\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "auCiX0EtMA2a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
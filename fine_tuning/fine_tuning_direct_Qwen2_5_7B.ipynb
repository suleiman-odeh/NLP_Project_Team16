{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suleiman-odeh/NLP_Project_Team16/blob/main/fine_tuning/fine_tuning_direct_Qwen2_5_7B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers peft trl bitsandbytes accelerate datasets"
      ],
      "metadata": {
        "id": "611qdMp81GHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AA22ubYTzqif",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9b10f21-a8d4-49ea-b72e-919ef37729cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ready to go.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Setup\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "\n",
        "print(\"Ready to go.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Base model\n",
        "model_name = \"Qwen/Qwen2.5-7B\"\n",
        "\n",
        "# Define 4-Bit Configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load Base Model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "# fix pad tokens\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Prepare for LoRA\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA Config according to the paper\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\", # text generation model\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "    )\n",
        "\n",
        "print(f\"Loaded BASE model: {model_name}\")"
      ],
      "metadata": {
        "id": "Xs227XcI3JaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load dataset\n",
        "filename = \"QEvasion_cleaned.jsonl\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_json(filename, lines=True)\n",
        "    print(f\"Loaded {len(df)} rows using Pandas.\")\n",
        "except ValueError:\n",
        "    # Fallback if it's a standard JSON list (not lines)\n",
        "    df = pd.read_json(filename)\n",
        "    print(f\"Loaded {len(df)} rows using Pandas (Standard JSON).\")\n",
        "\n",
        "# Filter train and test sets\n",
        "train_df = df[df['split_type'] == 'train']\n",
        "# We keep the test set safe for later\n",
        "test_df  = df[df['split_type'] == 'test']\n",
        "\n",
        "print(f\"   - Training pool: {len(train_df)}\")\n",
        "print(f\"   - Test pool: {len(test_df)}\")\n",
        "\n",
        "# convert to hugging face dataset\n",
        "full_train_dataset = Dataset.from_pandas(train_df)\n",
        "\n",
        "# SPLIT (TRAIN vs VALIDATION)\n",
        "# so it can match the paper\n",
        "dataset_split = full_train_dataset.train_test_split(test_size=0.2175, seed=42)\n",
        "\n",
        "train_dataset = dataset_split['train']\n",
        "eval_dataset = dataset_split['test']\n",
        "\n",
        "print(f\"Final Setup:\")\n",
        "print(f\"   - Training Samples: {len(train_dataset)}\")\n",
        "print(f\"   - Validation Samples: {len(eval_dataset)}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# prompt based on the paper\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    output_texts = []\n",
        "\n",
        "    instruction = (\n",
        "        \"Based on a part of the interview where the interviewer asks a set of questions, \"\n",
        "        \"classify the type of answer the interviewee provided for the following question \"\n",
        "        \"into one of these categories:\\n\"\n",
        "        \"1. Clear Reply - The information requested is explicitly stated (in the requested form)\\n\"\n",
        "        \"2. Clear Non-Reply - The information requested is not given at all due to ignorance, need for clarification or declining to answer\\n\"\n",
        "        \"3. Ambivalent - The information requested is given in an incomplete way e.g. the answer is too general, partial, implicit, dodging or deflection.\"\n",
        "    )\n",
        "\n",
        "    # Define columns\n",
        "    answers = examples['cleaned_answer']\n",
        "    questions = examples['question']\n",
        "    labels = examples['clarity_label']\n",
        "    is_batch = isinstance(questions, list)\n",
        "\n",
        "    if not is_batch:\n",
        "        questions = [questions]\n",
        "        answers = [answers]\n",
        "        labels = [labels]\n",
        "\n",
        "    for i in range(len(questions)):\n",
        "        # Get the label text\n",
        "        label_text = str(labels[i])\n",
        "\n",
        "        # Construct Prompt\n",
        "        text = f\"{instruction}\\n\\n### Part of the interview ###\\n{answers[i]}\\n\\n### Question ###\\n{questions[i]}\\n\\nLabel: {label_text}\"\n",
        "\n",
        "        # Add EOS token\n",
        "        text = text + tokenizer.eos_token\n",
        "        output_texts.append(text)\n",
        "    if not is_batch:\n",
        "        return output_texts[0]\n",
        "\n",
        "    return output_texts\n",
        "\n",
        "# VERIFY\n",
        "print(\"\\n--- Sample Training Input ---\")\n",
        "print(formatting_prompts_func(train_dataset[:1])[0])"
      ],
      "metadata": {
        "id": "73YoViaO_xjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Training Loop\n",
        "\"\"\"\n",
        "\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# sefine training arguments choice based on the settings used for OASST1 dataset, described in the paper\n",
        "# QLORA: Efficient Finetuning of Quantized LLMs\n",
        "training_args = SFTConfig(\n",
        "    output_dir=\"./qwen-finetuned-evasion\", # Where to save results\n",
        "\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,         # Accumulate to simulate batch_size=16\n",
        "\n",
        "    max_length=1024,\n",
        "    gradient_checkpointing=True,           # Saves huge memory\n",
        "    learning_rate=2e-4,                    # Standard QLoRA learning rate\n",
        "    num_train_epochs= 1,\n",
        "    lr_scheduler_type=\"constant\",          # from the paper\n",
        "    logging_steps=10,                      # Print stats every 10 steps\n",
        "    eval_strategy=\"steps\",           # Check Validation Set\n",
        "    eval_steps=50,                         # Check validation every 50 steps\n",
        "    save_strategy=\"no\",                    # Don't save intermediate checkpoints\n",
        "    fp16=False,                             # diffrent than the paper due to GPU constraints\n",
        "    bf16=True,\n",
        "    optim=\"paged_adamw_8bit\",              # 8-bit Optimizer for our Colab GPU\n",
        "    max_grad_norm=0.3,                     # from the paper\n",
        "    adam_beta2=0.999,                      # from the paper\n",
        "    report_to=\"none\"                       # Prevents login popups\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=peft_config,\n",
        "    formatting_func=formatting_prompts_func,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"Starting Training...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save the adapter\n",
        "trainer.model.save_pretrained(\"./final_adapter_qwen\")\n",
        "print(\"Training Complete. Adapter saved to './final_adapter_qwen'\")"
      ],
      "metadata": {
        "id": "bkDQ5pG2LwpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Inference: Check the testset\n",
        "\"\"\"\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "def generate_prediction(question, answer):\n",
        "    instruction = (\n",
        "        \"Based on a part of the interview where the interviewer asks a set of questions, \"\n",
        "        \"classify the type of answer the interviewee provided for the following question \"\n",
        "        \"into one of these categories:\\n\"\n",
        "        \"1. Clear Reply - The information requested is explicitly stated (in the requested form)\\n\"\n",
        "        \"2. Clear Non-Reply - The information requested is not given at all due to ignorance, need for clarification or declining to answer\\n\"\n",
        "        \"3. Ambivalent - The information requested is given in an incomplete way e.g. the answer is too general, partial, implicit, dodging or deflection.\"\n",
        "    )\n",
        "\n",
        "    # Construct prompt without the label\n",
        "    prompt = f\"{instruction}\\n\\n### Part of the interview ###\\n{answer}\\n\\n### Question ###\\n{question}\\n\\nLabel:\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=10,\n",
        "            temperature=0.1,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode only the new tokens\n",
        "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    prediction = full_text[len(prompt):].strip().split('\\n')[0]\n",
        "    return prediction\n",
        "\n",
        "# Generate predictions for the test set\n",
        "print(f\"Generating predictions for {len(test_df)} samples...\")\n",
        "test_df['predicted_label'] = [\n",
        "    generate_prediction(row['question'], row['cleaned_answer'])\n",
        "    for _, row in tqdm(test_df.iterrows(), total=len(test_df))\n",
        "]\n",
        "\n",
        "# Display a few results\n",
        "print(\"\\n--- Sample Predictions ---\")\n",
        "print(test_df[['question', 'clarity_label', 'predicted_label']].head())"
      ],
      "metadata": {
        "id": "LwvXsAznL6Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Evaluation\n",
        "\"\"\"\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure labels are treated as strings for comparison\n",
        "y_true = test_df['clarity_label'].astype(str).tolist()\n",
        "y_pred = test_df['predicted_label'].astype(str).tolist()\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f\"Overall Accuracy: {accuracy:.4f}\\n\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "# Confusion Matrix Visualization\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=sorted(set(y_true)),\n",
        "            yticklabels=sorted(set(y_true)))\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix: Qwen2.5 Evasion Classification')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "auCiX0EtMA2a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}